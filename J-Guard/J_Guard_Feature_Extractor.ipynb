{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install indic-nlp-library"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch7LXCHKVFUd",
        "outputId": "537f8e81-0898-4097-fde3-e1a457751363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m560.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.23.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.3.post1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.13.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.11.17)\n",
            "Installing collected packages: morfessor, sphinxcontrib-jquery, sphinx-rtd-theme, sphinx-argparse, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXyD3WcXT-es",
        "outputId": "1359d33d-9af8-4b82-a495-5e1847ab8290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-08 18:25:51.454401: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-08 18:25:51.454482: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-08 18:25:51.454528: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-08 18:25:53.600795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "import spacy\n",
        "# Spacy model imported\n",
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "from indicnlp.tokenize import sentence_tokenize #For sentence tokenization\n",
        "from indicnlp.tokenize import indic_tokenize #For word tokenization\n",
        "\n",
        "def word_count(document):\n",
        "\n",
        "  tokens = indic_tokenize.trivial_tokenize(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return len(filtered)\n",
        "\n",
        "def sentence_count(document):\n",
        "\n",
        "  tokens = sentence_tokenize.sentence_split(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return len(filtered)\n",
        "\n",
        "def paragraph_count(document):\n",
        "\n",
        "  tokens = document.splitlines()\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return len(filtered)\n",
        "\n",
        "def word_count_sent(document):\n",
        "\n",
        "  tokens = sentence_tokenize.sentence_split(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  word_counts = [word_count(sent) for sent in filtered]\n",
        "\n",
        "  if len(word_counts) ==0:\n",
        "\n",
        "    return 0, 0\n",
        "\n",
        "  mean = sum(word_counts) / len(word_counts)\n",
        "  variance = sum([((x - mean) ** 2) for x in word_counts]) / len(word_counts)\n",
        "  res = variance ** 0.5\n",
        "\n",
        "  return mean, res\n",
        "\n",
        "def word_count_para(document):\n",
        "\n",
        "  tokens = document.splitlines()\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  word_counts = [word_count(para) for para in filtered]\n",
        "\n",
        "  if len(word_counts) ==0:\n",
        "\n",
        "    return 0, 0\n",
        "\n",
        "  mean = sum(word_counts) / len(word_counts)\n",
        "  variance = sum([((x - mean) ** 2) for x in word_counts]) / len(word_counts)\n",
        "  res = variance ** 0.5\n",
        "\n",
        "  return mean, res\n",
        "\n",
        "def sent_count_para(document):\n",
        "\n",
        "  tokens = document.splitlines()\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  sent_counts = [sentence_count(para) for para in filtered]\n",
        "\n",
        "  if len(sent_counts) ==0:\n",
        "\n",
        "    return 0, 0\n",
        "\n",
        "  mean = sum(sent_counts) / len(sent_counts)\n",
        "  variance = sum([((x - mean) ** 2) for x in sent_counts]) / len(sent_counts)\n",
        "  res = variance ** 0.5\n",
        "\n",
        "  return mean, res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUnctuation Analysis"
      ],
      "metadata": {
        "id": "YopLC6FoUTfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def total_punc_count(document):\n",
        "\n",
        "  punct_count = 0\n",
        "\n",
        "  for char in document:\n",
        "\n",
        "    if char in string.punctuation:\n",
        "\n",
        "      punct_count +=1\n",
        "\n",
        "  return punct_count"
      ],
      "metadata": {
        "id": "KFPZA-YYUMo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def special_punc_count(document, special_puncts):\n",
        "\n",
        "  punct_count = []\n",
        "\n",
        "  for punct in special_puncts:\n",
        "\n",
        "    punct_count.append(document.count(punct))\n",
        "\n",
        "  total_puncts = total_punc_count(document)\n",
        "  if total_puncts==0:\n",
        "    return [0 for count in punct_count]\n",
        "  else:\n",
        "    return [float(count)/ total_puncts for count in punct_count]"
      ],
      "metadata": {
        "id": "0LDqseDdUXIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def special_punc_count_sent(document, special_puncts):\n",
        "\n",
        "  tokens = sentence_tokenize.sentence_split(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  punct_count = [0 for i in special_puncts] # Init as 0\n",
        "\n",
        "  if not filtered:\n",
        "    return punct_count\n",
        "\n",
        "  for sent in filtered:\n",
        "\n",
        "    for punct in special_puncts:\n",
        "\n",
        "      punct_count[special_puncts.index(punct)] += sent.count(punct)\n",
        "\n",
        "  return [float(count)/ len(filtered) for count in punct_count]\n",
        "\n",
        "\n",
        "def special_punc_count_para(document, special_puncts):\n",
        "\n",
        "  tokens = document.splitlines()\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  punct_count = [0 for i in special_puncts] # Init as 0\n",
        "\n",
        "  if not filtered:\n",
        "    return punct_count\n",
        "\n",
        "  for para in filtered:\n",
        "\n",
        "    for punct in special_puncts:\n",
        "\n",
        "      punct_count[special_puncts.index(punct)] += para.count(punct)\n",
        "\n",
        "  return [float(count)/ len(filtered) for count in punct_count]"
      ],
      "metadata": {
        "id": "Gy6lt563Ubm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Journalism Standard Analysis"
      ],
      "metadata": {
        "id": "rd-lycWfUeLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datefinder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW3OeflwUdtL",
        "outputId": "90bc5ec1-f332-424c-a633-c11915a33438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datefinder\n",
            "  Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.10/dist-packages (from datefinder) (2023.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from datefinder) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datefinder) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4.2->datefinder) (1.16.0)\n",
            "Installing collected packages: datefinder\n",
            "Successfully installed datefinder-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datefinder\n",
        "\n",
        "def word_count_lead_sent(document):\n",
        "\n",
        "  tokens = sentence_tokenize.sentence_split(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return word_count(filtered[0])\n",
        "\n",
        "\n",
        "def word_count_lead_para(document):\n",
        "\n",
        "  tokens = document.splitlines()\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  return word_count(filtered[0])\n",
        "\n",
        "\n",
        "def number_count(document):\n",
        "\n",
        "  total_digits = 0\n",
        "\n",
        "  tokens = indic_tokenize.trivial_tokenize(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  for s in filtered:\n",
        "\n",
        "    if s.isnumeric():\n",
        "\n",
        "      try:\n",
        "        num = int(s)\n",
        "\n",
        "        if num <9:\n",
        "\n",
        "          total_digits += 1\n",
        "\n",
        "      except:\n",
        "\n",
        "        pass\n",
        "\n",
        "  return total_digits\n",
        "\n",
        "\n",
        "def number_count_para(document, avg=True):\n",
        "\n",
        "  tokens = document.splitlines()\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  num_counts = [number_count(para) for para in filtered]\n",
        "\n",
        "\n",
        "  if avg:\n",
        "    if len(num_counts) ==0:\n",
        "\n",
        "      return 0, 0\n",
        "\n",
        "    mean = sum(num_counts) / len(num_counts)\n",
        "\n",
        "    return mean\n",
        "\n",
        "  else:\n",
        "\n",
        "    return sum(num_counts)\n",
        "\n",
        "\n",
        "def passive_sent_count(document, avg=True):\n",
        "\n",
        "  tokens = sentence_tokenize.sentence_split(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  passive_count = [is_passive(sent) for sent in filtered]\n",
        "\n",
        "  if avg:\n",
        "    return sum(passive_count)/len(passive_count)\n",
        "  else:\n",
        "    return sum(passive_count)\n",
        "\n",
        "\n",
        "def past_sent_count(document, avg=True):\n",
        "\n",
        "  tokens = sentence_tokenize.sentence_split(document, lang='hi')\n",
        "\n",
        "  nonPunct = re.compile('.*[A-Za-z0-9ऀ-ॿ].*')  # must contain a letter or digit\n",
        "\n",
        "  filtered = [w for w in tokens if nonPunct.match(w)]\n",
        "\n",
        "  past_count = [is_past(sent) for sent in filtered]\n",
        "\n",
        "  if avg:\n",
        "    return sum(past_count)/len(past_count)\n",
        "  else:\n",
        "    return sum(past_count)\n",
        "\n",
        "\n",
        "# function to check the type of sentence\n",
        "def is_passive(inputSentence):\n",
        "    # running the model on sentence\n",
        "    doc_file = nlp(inputSentence)\n",
        "\n",
        "    # getting the syntactic dependency\n",
        "    all_tags = [token.dep_ for token in doc_file]\n",
        "\n",
        "    # checking for 'agent' tag\n",
        "    agent_test = any(['agent' in sublist for sublist in all_tags])\n",
        "\n",
        "    # checking for 'nsubjpass' tag\n",
        "    nsubjpass_test = any(['nsubjpass' in sublist for sublist in all_tags])\n",
        "\n",
        "    return agent_test or nsubjpass_test\n",
        "\n",
        "\n",
        "# function to check the past tense words of sentence\n",
        "def is_past(inputSentence):\n",
        "    # running the model on sentence\n",
        "\n",
        "    text = inputSentence.split()\n",
        "\n",
        "    tokens_tag = pos_tag(text)\n",
        "\n",
        "    all_tokens = [i[1] for i in tokens_tag]\n",
        "\n",
        "    if \"VBD\" in all_tokens or \"VBN\" in all_tokens:\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_temporal_pharses(document):\n",
        "\n",
        "  matches = datefinder.find_dates(document, source=True, index=True)\n",
        "\n",
        "  return [match[1] for match in matches]\n",
        "\n",
        "\n",
        "def is_time(time_string, std = False):\n",
        "\n",
        "  if std:\n",
        "    time_postfix = [\"a.m.\", \"p.m.\"]\n",
        "\n",
        "  else:\n",
        "\n",
        "    time_string = time_string.lower()\n",
        "    time_postfix = [\"a.m.\", \"p.m.\", \"am\", \"pm\", \"am.\", \"pm.\", \"a.m\", \"p.m\"]\n",
        "\n",
        "  regex_dates = [r'{}'.format(word) for word in time_postfix]\n",
        "\n",
        "  for reg in regex_dates:\n",
        "\n",
        "    match = re.search(reg , time_string)\n",
        "\n",
        "    if match:\n",
        "\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def is_day(time_string, std = False):\n",
        "\n",
        "  if std:\n",
        "    day_postfix = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "  else:\n",
        "\n",
        "    time_string = time_string.lower()\n",
        "    day_postfix = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\",\n",
        "                   \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Thur\" \"Fri\", \"Sat\", \"Sun\"]\n",
        "\n",
        "  regex_dates = [r'{}'.format(word) for word in day_postfix]\n",
        "\n",
        "  for reg in regex_dates:\n",
        "\n",
        "    match = re.search(reg , time_string)\n",
        "\n",
        "    if match:\n",
        "\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def is_month(time_string, std = False):\n",
        "\n",
        "  if std:\n",
        "\n",
        "    months = [\"March\", \"April\", \"May\", \"June\", \"July\",\n",
        "              \"Jan.\", \"Feb.\",\"Aug.\", \"Sept.\", \"Oct.\", \"Nov.\",  \"Dec.\"]\n",
        "\n",
        "  else:\n",
        "    time_string = time_string.lower()\n",
        "\n",
        "    months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\",\n",
        "              \"Jan\", \"Feb\", \"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\", \"Sept\", \"Oct\", \"Nov\",  \"Dec\"]\n",
        "\n",
        "\n",
        "  regex_dates = [r'{}'.format(word) for word in months]\n",
        "\n",
        "  for reg in regex_dates:\n",
        "\n",
        "    match = re.search(reg , time_string)\n",
        "\n",
        "    if match:\n",
        "\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def temporal_style_inconsit_count(document):\n",
        "\n",
        "  temp_obj_list = get_temporal_pharses(document)\n",
        "\n",
        "  violation_count = 0\n",
        "\n",
        "  for temp_obj in temp_obj_list:\n",
        "\n",
        "    if is_time(temp_obj):\n",
        "\n",
        "      if not is_time(temp_obj, std=True):\n",
        "\n",
        "        violation_count +=1\n",
        "\n",
        "    if is_day(temp_obj):\n",
        "\n",
        "      if not is_day(temp_obj, std=True):\n",
        "\n",
        "        violation_count +=1\n",
        "\n",
        "    if is_month(temp_obj):\n",
        "\n",
        "      if not is_month(temp_obj, std=True):\n",
        "\n",
        "        violation_count +=1\n",
        "\n",
        "  return violation_count\n"
      ],
      "metadata": {
        "id": "DyJki4H7UiLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Exatrction\n",
        "## Combined function to get journalism features"
      ],
      "metadata": {
        "id": "GxynaKgpUrg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "def get_features(data):\n",
        "\n",
        "  global phraseology_features\n",
        "  global special_puncts\n",
        "\n",
        "  global special_punct_names\n",
        "\n",
        "  global punct_analysis_features\n",
        "\n",
        "  global style_guide_features\n",
        "\n",
        "  global frame_cols\n",
        "\n",
        "  global data_features\n",
        "\n",
        "  phraseology_features = [\"word_count\", \"sent_count\", \"para_count\", \"mean_word_count_sent\", \"std_word_count_sent\",\"mean_word_count_para\", \"std_word_count_para\", \"mean_sent_count_para\", \"std_sent_count_para\"]\n",
        "  special_puncts = [\"!\",\"'\", \",\", \"-\", \":\", \";\", \"?\", \"@\", \"\\\"\", \"=\", \"#\"]\n",
        "\n",
        "  special_punct_names = [\"excla\",\"apos\", \"comma\", \"hypn\", \"col\", \"semicol\", \"ques\", \"at\", \"qot\", \"dhypn\", \"hash\"]\n",
        "\n",
        "  punct_analysis_features = [\"total_punct_count\"]\n",
        "\n",
        "  style_guide_features = [\"wc_lead_sent\", \"wc_lead_para\", \"num_count\", \"passive_sent_count\", \"past_tense_count\", \"temp_inconsis\"]\n",
        "\n",
        "  frame_cols = []\n",
        "\n",
        "  data_features = []\n",
        "\n",
        "  data_features = []\n",
        "\n",
        "  for punct in special_punct_names:\n",
        "\n",
        "    punct_analysis_features.append(punct + \"_mean_count\")\n",
        "\n",
        "  for punct in special_punct_names:\n",
        "\n",
        "    punct_analysis_features.append(punct + \"_mean_count_sent\")\n",
        "\n",
        "  for punct in special_punct_names:\n",
        "\n",
        "    punct_analysis_features.append(punct + \"_mean_count_para\")\n",
        "\n",
        "\n",
        "  for value in tqdm(data.itertuples()):\n",
        "\n",
        "    document = str(value.text)\n",
        "\n",
        "    if not document:\n",
        "\n",
        "      document = \"empty\"\n",
        "\n",
        "    feature_row = []\n",
        "    ## phraseology features\n",
        "    # print(document)\n",
        "    feature_row.append(word_count(document))\n",
        "    feature_row.append(sentence_count(document))\n",
        "    feature_row.append(paragraph_count(document))\n",
        "\n",
        "    # word count per sentence\n",
        "    word_count_vals = word_count_sent(document)\n",
        "    feature_row.append(word_count_vals[0])\n",
        "    feature_row.append(word_count_vals[1])\n",
        "\n",
        "    # word count per paragraph\n",
        "    word_count_vals = word_count_para(document)\n",
        "    feature_row.append(word_count_vals[0])\n",
        "    feature_row.append(word_count_vals[1])\n",
        "\n",
        "    # sentence count per paragraph\n",
        "    sent_count_vals = sent_count_para(document)\n",
        "    feature_row.append(sent_count_vals[0])\n",
        "    feature_row.append(sent_count_vals[1])\n",
        "\n",
        "    ## punctuation features\n",
        "    feature_row.append(total_punc_count(document))\n",
        "    feature_row.extend(special_punc_count(document, special_puncts))\n",
        "    feature_row.extend(special_punc_count_sent(document, special_puncts))\n",
        "    feature_row.extend(special_punc_count_para(document, special_puncts))\n",
        "\n",
        "\n",
        "    ## Style guide features\n",
        "    feature_row.append(word_count_lead_sent(document))\n",
        "    feature_row.append(word_count_lead_para(document))\n",
        "    feature_row.append(number_count_para(document))\n",
        "    feature_row.append(passive_sent_count(document))\n",
        "    feature_row.append(past_sent_count(document))\n",
        "    feature_row.append(temporal_style_inconsit_count(document))\n",
        "\n",
        "    # append label\n",
        "    # feature_row.append(value.label)\n",
        "    data_features.append(feature_row)\n",
        "\n",
        "  frame_cols = phraseology_features.copy()\n",
        "  frame_cols.extend(punct_analysis_features)\n",
        "  frame_cols.extend(style_guide_features)\n",
        "  # frame_cols.append('label')\n",
        "\n",
        "  # print(\"length of feature vector (column names) \")\n",
        "  # print(len(frame_cols))\n",
        "  print(frame_cols)\n",
        "\n",
        "  data_features = pd.DataFrame(data_features, columns=frame_cols)\n",
        "  return data_features"
      ],
      "metadata": {
        "id": "OIEHT7_qUoEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "gaFIKPJEXs7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"\") # Enter dataset file\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BioAUsFNWE4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features\n",
        "features = get_features(df)"
      ],
      "metadata": {
        "id": "bL2spFBCXotd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the file\n",
        "features.to_csv('', index=False)"
      ],
      "metadata": {
        "id": "zAU7kFh6ZTqU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}